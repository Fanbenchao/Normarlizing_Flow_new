{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import logging\n",
    "import time\n",
    "import socket\n",
    "import random\n",
    "from datetime import datetime\n",
    "from data_gener.data_generator import process\n",
    "from models.utils import ResultLogger,hps_logger,get_optimizer\n",
    "from utils.mylogger import add_logging_level\n",
    "from models.noise_flow_model import NoiseFlow\n",
    "from utils.ArgParser import arg_parser\n",
    "from utils.sidd_utils import init_params,restore_last_model\n",
    "from models.train_sample import sample,train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_index = [4, 11, 13, 17, 18, 20, 22, 23, 25, 27, 28, 29, 30, 34, 35, 39, 40, 42, 43, 44, 45, 47, 81, 86, 88,\n",
    "#                      90, 101, 102, 104, 105, 110, 111, 115, 116, 125, 126, 127, 129, 132, 135,\n",
    "#                      138, 140, 175, 177, 178, 179, 180, 181, 185, 186, 189, 192, 193, 194, 196, 197]\n",
    "# test_index = [54, 55, 57, 59, 60, 62, 63, 66, 150, 151, 152, 154, 155, 159, 160, 161, 163, 164, 165, 166, 198,\n",
    "#                      199]\n",
    "# train_index = [154, 146, 160, 173, 189, 135, 106, 164, 102, 168, 126, 190, 181,186, 165, 151, 161, 140, 149,  29,  55,  59,   1,   6,  45,   5,\n",
    "#                 51,   4,  22,   3, 101, 122,  88,  94, 167, 129, 150,   8, 163, 7, 105, 180,  86,  63, 147, 159, 188, 172,  48, 155,  18,  34,\n",
    "#                 28,  33, 132, 197, 114,  66, 198, 107, 144, 137, 166, 157,  64]\n",
    "# test_index = [111, 185, 130,  99, 156, 136,  32,  17,  25,  98, 125, 110, 134, 138, 184, 142,  60,  54, 192, 152, 191]\n",
    "train_index = [4, 11, 13, 17, 18, 20, 22, 23, 25, 27, 28, 29, 30, 34, 35, 39, 40, 42, 43, 44, 45, 47, 81, 86, 88,\n",
    "                     90, 101, 102, 104, 105, 110, 111, 115, 116, 125, 126, 127, 129, 132, 135,\n",
    "                     138, 140, 175, 177, 178, 179, 180, 181, 185, 186, 189, 192, 193, 194, 196, 197]\n",
    "test_index = [54, 55, 57, 59, 60, 62, 63, 66, 150, 151, 152, 154, 155, 159, 160, 161, 163, 164, 165, 166, 198,\n",
    "                     199]\n",
    "data_path = '/home/lupin/SIDD_Small_Raw_Only/Data'\n",
    "patch_height = 32\n",
    "decay_steps = 1000\n",
    "decay_rate = 0.9\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = process(data_path,train_index,patch_height)\n",
    "test_list = process(data_path,test_index,patch_height)\n",
    "train_step = int(np.ceil(len(train_list)/batch_size))\n",
    "test_step = int(np.ceil(len(test_list)/batch_size))\n",
    "random.shuffle(train_list)\n",
    "random.shuffle(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hps = arg_parser()\n",
    "hps.continue_training = False\n",
    "total_time = time.time()\n",
    "host = socket.gethostname()\n",
    "tf.compat.v1.set_random_seed(hps.seed)\n",
    "np.random.seed(hps.seed)\n",
    "logdir = os.path.abspath(os.path.join('experiments', hps.problem, hps.logdir)) + '/'\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir, exist_ok=True)\n",
    "hps.logdirname = hps.logdir\n",
    "hps.logdir = logdir\n",
    "# set up a custom logger\n",
    "add_logging_level('TRACE', 100)\n",
    "logging.getLogger(__name__).setLevel(\"TRACE\")\n",
    "logging.basicConfig(level=logging.TRACE)\n",
    "\n",
    "x_shape = [None, patch_height, patch_height, 4]\n",
    "hps.x_shape = x_shape\n",
    "hps.n_dims = np.prod(x_shape[1:])\n",
    "\n",
    "input_shape = x_shape\n",
    "\n",
    "# Build noise flow graph\n",
    "logging.trace('Building NoiseFlow...')\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "is_training = tf.compat.v1.placeholder(tf.bool, name='is_training')\n",
    "x = tf.compat.v1.placeholder(tf.float32, x_shape, name='noise_image')\n",
    "y = tf.compat.v1.placeholder(tf.float32, x_shape, name='clean_image')\n",
    "nlf0 = tf.compat.v1.placeholder(tf.float32, [None], name='nlf0')\n",
    "nlf1 = tf.compat.v1.placeholder(tf.float32, [None], name='nlf1')\n",
    "iso = tf.compat.v1.placeholder(tf.float32, [None], name='iso')\n",
    "shutter = tf.compat.v1.placeholder(tf.float32, [None], name='shutter')\n",
    "lr = tf.compat.v1.placeholder(tf.float32, None, name='learning_rate')\n",
    "\n",
    "# initialization of signal, gain, and camera parameters\n",
    "if hps.sidd_cond == 'mix':\n",
    "    init_params(hps)\n",
    "\n",
    "# NoiseFlow model\n",
    "nf = NoiseFlow(input_shape[1:], is_training, hps)\n",
    "loss_val, sd_z = nf.loss(x, y, nlf0=nlf0, nlf1=nlf1, iso=iso, shutter = shutter)\n",
    "\n",
    "# save variable names and number of parameters\n",
    "vs = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)\n",
    "vars_files = os.path.join(hps.logdir, 'model_vars.txt')\n",
    "with open(vars_files, 'w') as vf:\n",
    "    vf.write(str(vs))\n",
    "hps.num_params = int(np.sum([np.prod(v.get_shape().as_list())\n",
    "                             for v in tf.compat.v1.trainable_variables()]))\n",
    "logging.trace('number of parameters = %d' % hps.num_params)\n",
    "hps_logger(hps.logdir + 'hps.txt', hps, nf.get_layer_names(), hps.num_params)\n",
    "\n",
    "# create session\n",
    "sess = tf.compat.v1.Session()\n",
    "\n",
    "# create a saver.\n",
    "saver = tf.compat.v1.train.Saver(max_to_keep=0)  # keep all models\n",
    "\n",
    "# checkpoint directory\n",
    "ckpt_dir = os.path.join(hps.logdir, 'ckpt')\n",
    "ckpt_path = os.path.join(ckpt_dir, 'model.ckpt')\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "# sampling temperature (default = 1.0)\n",
    "if hps.temp is None:\n",
    "    hps.temp = 1.0\n",
    "\n",
    "# setup the output log\n",
    "train_logger = test_logger = None\n",
    "log_columns = ['epoch', 'NLL','sdz','train_time']\n",
    "# NLL: negative log likelihood\n",
    "# sdz: standard deviation of the base measure (sanity check)\n",
    "train_logger = ResultLogger(hps.logdir + 'train.txt', log_columns, hps.continue_training)\n",
    "sample_logger = ResultLogger(hps.logdir + 'sample.txt', ['sample_time','KLD_G', 'KLD_NLF', 'KLD_NF', 'KLD_R'],hps.continue_training)\n",
    "\n",
    "tcurr = time.time()                      \n",
    "# continue training?\n",
    "start_epoch = 1\n",
    "logging.trace('continue_training = ' + str(hps.continue_training))\n",
    "if hps.continue_training:\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    saver.restore(sess, path.join(ckpt_dir,'model.ckpt.best'))\n",
    "    last_epoch = restore_last_model(ckpt_dir, sess, saver)\n",
    "    start_epoch = 1 + last_epoch\n",
    "    # noinspection PyBroadException\n",
    "    try:\n",
    "        train_op = tf.compat.v1.get_collection('train_op')  # [0]\n",
    "    except:\n",
    "        logging.trace('could not restore optimizer state, preparing a new optimizer')\n",
    "        train_op = get_optimizer(hps, lr, loss_val,decay_steps,decay_rate)\n",
    "else:\n",
    "    logging.trace('preparing optimizer')\n",
    "    train_op = get_optimizer(hps, lr, loss_val,decay_steps,decay_rate)\n",
    "    logging.trace('initializing variables')\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "# Epochs\n",
    "logging.trace('Starting training/testing/samplings.')\n",
    "logging.trace('Logging to ' + hps.logdir)\n",
    "kldiv_best = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, hps.epochs + 1):\n",
    "\n",
    "    # Testing\n",
    "    if (epoch < 10 or (epoch < 100 and epoch % 10 == 0) or epoch % hps.epochs_full_valid == 0.):\n",
    "        kldiv3, t_sample = sample(sess,nf,test_list,x,y,iso,shutter,is_training,batch_size,test_step,hps,sample_logger,epoch)\n",
    "        saver.save(sess, ckpt_path, global_step=epoch)\n",
    "        # best model?\n",
    "        if  kldiv3[2]< kldiv_best:\n",
    "            kldiv_best = kldiv3[2]\n",
    "            saver.save(sess, ckpt_path + '.best')\n",
    "            is_best = 1\n",
    "        else:\n",
    "            is_best = 0\n",
    "    t_curr = 0\n",
    "    sd_z_tr,train_loss,t_train = train(sess,train_list,loss_val,sd_z,train_op,x,y,nlf0,nlf1,iso,shutter,lr,\n",
    "                                       is_training,batch_size,train_step,hps,train_logger,epoch)\n",
    "    t_curr = time.time()-tcurr\n",
    "    tcurr = time.time()\n",
    "    # Training loop\n",
    "    \n",
    "    # End training\n",
    "\n",
    "    # print results of train/test/sample\n",
    "    tr_l = train_loss\n",
    "    if epoch < 10 or (epoch < 100 and epoch % 10 == 0) or \\\n",
    "            epoch % hps.epochs_full_valid == 0.:\n",
    "        # E: epoch\n",
    "        # tr, ts, tsm, tv: time of training, testing, sampling, visualization\n",
    "        # T: total time\n",
    "        # tL, sL, smL: loss of training, testing, sampling\n",
    "        # SDr, SDs: std. dev. of base measure in training and testing\n",
    "        # B: 1 if best model, 0 otherwise\n",
    "        print('%s %s %s Epoch=%d train time=%.1f  sample time=%.1f T=%.1f '\n",
    "              'loss=%5.1f SDr=%.1f B=%d' %\n",
    "              (str(datetime.now())[11:16], host, hps.logdirname, epoch, t_train,t_sample, t_curr,\n",
    "               tr_l, sd_z_tr, is_best),end='')\n",
    "        if kldiv3 is not None:\n",
    "            print(' ', end='')\n",
    "            # marginal KL divergence of noise samples from: Gaussian, camera-NLF, and NoiseFlow, respectively\n",
    "            print(','.join('{0:.3f}'.format(kk) for kk in kldiv3), end='')\n",
    "        print('', flush=True)\n",
    "\n",
    "total_time = time.time() - total_time\n",
    "logging.trace('Total time = %f' % total_time)\n",
    "with open(path.join(hps.logdir, 'total_time.txt'), 'w') as f:\n",
    "    f.write('total_time (s) = %f' % total_time)\n",
    "logging.trace(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
